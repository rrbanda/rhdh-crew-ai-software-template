# **üöÄ Leopard Pont Des Arts API**

**Leopard Pont Des Arts** is an **AI-powered agent** built using **CrewAI**, integrating **real-time web search** and **large language models (LLMs)** to generate intelligent responses.

This AI assistant:
- ‚úÖ **Fetches real-time information** via **DuckDuckGo search**
- ‚úÖ **Processes search & input data using an LLM** (Supports vLLM, OpenAI, Ollama, DeepSeek, and more)
- ‚úÖ **Uses CrewAI for structured AI workflows**
- ‚úÖ **Supports multiple deployment options** (Local, Podman, OpenShift)

It is designed for **automated research, AI-enhanced decision-making, and knowledge retrieval**.

---

## **üìå Features**
- ‚úÖ **Multi-LLM Provider Support** ‚Äì Works with OpenAI, vLLM, Ollama, DeepSeek, Cohere, Mistral, Anthropic, Gemini, Meta, and more
- ‚úÖ **CrewAI Agent Framework** ‚Äì Implements structured AI-driven responses
- ‚úÖ **DuckDuckGo Search Integration** ‚Äì Enhances AI answers with **live search results**
- ‚úÖ **FastAPI-based REST API** ‚Äì Easily extendable for additional AI workflows
- ‚úÖ **Environment Config Support** ‚Äì Works with `.env` or Kubernetes `ConfigMap`
- ‚úÖ **Podman Desktop & OpenShift Ready** ‚Äì Supports both **containerized and cloud-based deployments**
- ‚úÖ **Works on Any OS** ‚Äì Uses **virtual environments (venv)** and **Podman**, avoiding OS dependencies

---

## **üõ†Ô∏è Setup Instructions**

### **1Ô∏è‚É£ Local Development (Without Podman)**
#### **üîπ Prerequisites**
- **Python 3.11+**
- **pip & virtualenv**

#### **üîπ Install & Run**
```bash
# Clone the repository
git clone https://github.com/your-repo/leopard_pontdesarts.git
cd leopard_pontdesarts

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate      # Windows

# Install dependencies
pip install -r requirements.txt

# Create a `.env` file with your API credentials
cp .env.example .env
nano .env  # Update API keys and model details

# Run the API on port 8082
PORT=8082 python -m src.main --mode api
```

#### **üîπ Test the API (cURL Examples)**
```bash
curl -X GET http://127.0.0.1:8082/
curl -X GET http://127.0.0.1:8082/leopard-crossing
```

---

### **2Ô∏è‚É£ Run with Podman**
#### **üîπ Prerequisites**
- **[Podman Installed](https://podman.io/getting-started/installation)**
- **Podman Desktop (Optional, for GUI management)**

#### **üîπ Build & Run with Podman**
```bash
# Build the container
podman build -t quay.io/yourusername/leopard_pontdesarts:latest .

# Run the container with .env file (Port 8082)
podman run --env-file .env -p 8082:8000 quay.io/yourusername/leopard_pontdesarts:latest

# OR Run with inline -e parameters
podman run -p 8082:8000 \
  -e LLM_PROVIDER="vllm" \
  -e LLM_BASE_URL="http://localhost:8000" \
  -e LLM_MODEL="/var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct" \
  quay.io/yourusername/leopard_pontdesarts:latest
```

---

### **3Ô∏è‚É£ Deploy via Podman Desktop (Kube YAML)**
#### **üîπ Steps**
1. **Create the Pod & ConfigMap** using the provided `pod.yaml`
2. **Apply YAML in Podman Desktop**
   ```bash
   podman kube play pod.yaml
   ```
3. **Check Running Containers**
   ```bash
   podman ps -a
   ```

#### **üîπ Access the API**
```bash
curl -X GET http://localhost:8082/
curl -X GET http://localhost:8082/leopard-crossing
```

---

## **üìÑ API Endpoints**
| Method | Endpoint                   | Description                         |
|--------|----------------------------|-------------------------------------|
| GET    | `/`                        | API health check                   |
| GET    | `/leopard-crossing`        | Retrieves AI-generated response    |
| GET    | `/leopard-crossing-ui`     | Fetches a structured response via UI |

---

## **üì¶ Environment Configuration**
The app reads values from `.env` or a `ConfigMap`.

### **‚úÖ `.env` Example**
```ini
LLM_PROVIDER="vllm"  # Change to "openai", "ollama", etc.
LLM_BASE_URL="http://localhost:8000"
LLM_MODEL="/var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct"
LLM_API_KEY="your-secret-key"  # If required
FORMATTER_API_URL="http://localhost:8001/process"
CHROMA_DB_PATH="/opt/app-root/src/.local/chroma_db"
LOG_LEVEL="INFO"
```

### **‚úÖ `ConfigMap` Equivalent (for Kubernetes/Podman)**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: leopard-config
data:
  LLM_PROVIDER: "vllm"
  LLM_BASE_URL: "http://localhost:8000"
  LLM_MODEL: "/var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct"
  LLM_API_KEY: "your-secret-key"
  FORMATTER_API_URL: "http://localhost:8001/process"
  CHROMA_DB_PATH: "/opt/app-root/src/.local/chroma_db"
  LOG_LEVEL: "INFO"
```

---

## **üöÄ Example Run Output**
Below is an example output when running the application using **Podman** with the **Ollama LLM provider** and a locally hosted model.

### **üîπ Run the Application with Podman**
```bash
podman run -p 8082:8000 \
  -e LLM_PROVIDER="ollama" \
  -e LLM_BASE_URL="http://localhost:8000" \
  -e LLM_MODEL="/var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct" \
  quay.io/yourusername/leopard_pontdesarts:latest
```

### **üîπ Console Output**
```
2025-02-03 01:10:27,748 - INFO - üîç Loaded LLM_MODEL: /var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct
2025-02-03 01:10:27,749 - INFO - üîç Loaded LLM_BASE_URL: http://localhost:8000
2025-02-03 01:10:27,749 - INFO - üîç LLM_API_KEY: SET
INFO:   Started server process [1]
...
2025-02-03 01:10:44,079 - INFO - ‚úÖ Using LLM Provider: ollama | Model: /var/home/instruct/.cache/instructlab/models/Qwen/Qwen2.5-Coder-32B-Instruct
...
```

### **üîπ LLM API Response**
```json
{
  "time_seconds": 9.44,
  "explanation": "The time taken for a leopard running at 58 km/h to cross a 155-meter bridge is calculated by converting the speed to m/s and then using the formula Time = Distance / Speed."
}
```

### **üîπ API Call Example**
Once the application is running, you can test the `/leopard-crossing` API endpoint:
```bash
curl -X GET http://127.0.0.1:8082/leopard-crossing
```

Expected **response:**
```json
{
  "time_seconds": 9.44,
  "explanation": "The time taken for a leopard running at 58 km/h to cross a 155-meter bridge is calculated by converting the speed to m/s and then using the formula Time = Distance / Speed."
}
```

---

## **üéØ Summary**
| Mode               | Command |
|--------------------|---------|
| **Local (No Podman)** | `PORT=8082 python -m src.main --mode api` |
| **Podman CLI (Using .env)** | `podman run --env-file .env -p 8082:8000 quay.io/yourusername/leopard_pontdesarts:latest` |
| **Podman CLI (Inline -e Variables)** | `podman run -p 8082:8000 -e LLM_PROVIDER="vllm" -e LLM_BASE_URL="http://localhost:8000" -e LLM_MODEL="..." quay.io/yourusername/leopard_pontdesarts:latest` |
| **Podman Desktop (Kube)** | `podman kube play pod.yaml` |

---

